{
 "nbformat": 3, 
 "nbformat_minor": 0, 
 "worksheets": [
  {
   "cells": [
    {
     "source": [
      "> This is one of the 100 recipes of the [IPython Cookbook](http://ipython-books.github.io/), the definitive guide to high-performance scientific computing and data science in Python.\n"
     ], 
     "cell_type": "markdown", 
     "metadata": []
    }, 
    {
     "source": [
      "# 7.3. Getting started with Bayesian methods\n", 
      "\n", 
      "In the last recipe, we used a frequentist method to test a hypothesis on incomplete data. Here, we will see an alternative approach based on **Bayesian theory**. The main idea is to consider that *unknown parameters are random variables*, just like the variables describing the experiment. Prior knowledge about the parameters is integrated in the model. This knowledge is updated as more and more data is observed.\n", 
      "\n", 
      "Frequentists and Bayesians interpret probabilities differently. Frequentists interpret a probability as a limit of frequencies when the number of samples tends to infinity. Bayesians interpret it as a belief, which is updated as more and more data is observed.\n", 
      "\n", 
      "Here, we revisit the previous coin flipping example with a Bayesian approach. This example is sufficiently simple to admit an analytical treatment. In general, as we will see later in this chapter, analytical results cannot be obtained and numerical methods become essential."
     ], 
     "cell_type": "markdown", 
     "metadata": {
      "word_id": "4818_07_bayesian"
     }
    }, 
    {
     "source": [
      "## Getting started\n", 
      "\n", 
      "Knowledge of basic probability theory (random variables, distributions, Bayes formula...) and real analysis (derivatives, integrals...) is recommended. We use the same notations as in the previous recipe."
     ], 
     "cell_type": "markdown", 
     "metadata": {}
    }, 
    {
     "source": [
      "## How to do it...\n", 
      "\n", 
      "Let $q$ be the probability of obtaining a head. Whereas $q$ was just a fixed number in the previous recipe, we consider here that it is a *random variable*. Initially, this variable follows a distribution called the **prior distribution**. It represents our knowledge about $q$ *before* we start flipping the coin. We will update this distribution after each trial (**posterior distribution**)."
     ], 
     "cell_type": "markdown", 
     "metadata": {}
    }, 
    {
     "source": [
      "1. First, we assume that $q$ is a *uniform* random variable on the interval $[0, 1]$. That's our prior distribution: for all $q$, $P(q)=1$.\n", 
      "2. Then, we flip our coin $n$ times. We note $x_i$ the outcome of the $i$-th flip ($0$ for tail, $1$ for head).\n", 
      "3. What is the probability distribution of $q$ knowing the observations $x_i$? **Bayes' formula** allows us to compute the *posterior distribution* analytically (see the next section for the mathematical details):\n", 
      "\n", 
      "$$P(q | \\{x_i\\}) = \\frac{P(\\{x_i\\} | q) P(q)}{\\displaystyle\\int_0^1 P(\\{x_i\\} | q) P(q) dq} = (n+1)\\binom n h  q^h (1-q)^{n-h}$$"
     ], 
     "cell_type": "markdown", 
     "metadata": {}
    }, 
    {
     "source": [
      "We define the posterior distribution according to the mathematical formula above. We remark this this expression is $(n+1)$ times the *probability mass function* (PMF) of the binomial distribution, which is directly available in `scipy.stats`. (http://en.wikipedia.org/wiki/Binomial_distribution)"
     ], 
     "cell_type": "markdown", 
     "metadata": {}
    }, 
    {
     "cell_type": "code", 
     "language": "python", 
     "outputs": [], 
     "collapsed": false, 
     "input": [
      "import numpy as np\n", 
      "import scipy.stats as st\n", 
      "import matplotlib.pyplot as plt\n", 
      "%matplotlib inline"
     ], 
     "metadata": {}
    }, 
    {
     "cell_type": "code", 
     "language": "python", 
     "outputs": [], 
     "collapsed": false, 
     "input": [
      "posterior = lambda n, h, q: (n+1) * st.binom(n, q).pmf(h)"
     ], 
     "metadata": {}
    }, 
    {
     "source": [
      "Let's plot this distribution for an observation of $h=61$ heads and $n=100$ total flips."
     ], 
     "cell_type": "markdown", 
     "metadata": {}
    }, 
    {
     "cell_type": "code", 
     "language": "python", 
     "outputs": [], 
     "collapsed": false, 
     "input": [
      "n = 100\n", 
      "h = 61\n", 
      "q = np.linspace(0., 1., 1000)\n", 
      "d = posterior(n, h, q)"
     ], 
     "metadata": {}
    }, 
    {
     "cell_type": "code", 
     "language": "python", 
     "outputs": [], 
     "collapsed": false, 
     "input": [
      "plt.figure(figsize=(5,3));\n", 
      "plt.plot(q, d, '-k');\n", 
      "plt.xlabel('q parameter');\n", 
      "plt.ylabel('Posterior distribution');\n", 
      "plt.ylim(0, d.max()+1);"
     ], 
     "metadata": {}
    }, 
    {
     "source": [
      "4. This distribution indicates the plausible values for $q$ given the observations. We could use it to derive a **credible interval**, likely to contain the actual value. (http://en.wikipedia.org/wiki/Credible_interval)\n", 
      "\n", 
      "We can also derive a point estimate. For example, the **maximum a posteriori (MAP) estimation** consists in considering the *maximum* of this distribution as an estimate for $q$. We can find this maximum analytically or numerically. Here, we find analytically $\\hat q = h/n$, which looks quite sensible. (http://en.wikipedia.org/wiki/Maximum_a_posteriori_estimation)"
     ], 
     "cell_type": "markdown", 
     "metadata": {}
    }, 
    {
     "source": [
      "> You'll find all the explanations, figures, references, and much more in the book (to be released later this summer).\n\n> [IPython Cookbook](http://ipython-books.github.io/), by [Cyrille Rossant](http://cyrille.rossant.net), Packt Publishing, 2014 (400 pages). [Get a 50% discount by pre-ordering now](http://www.packtpub.com/ipython-interactive-computing-and-visualization-cookbook/book) with the code `PICVCEB` (time-limited offer)!\n"
     ], 
     "cell_type": "markdown", 
     "metadata": {}
    }
   ], 
   "metadata": {}
  }
 ], 
 "metadata": {
  "name": "", 
  "signature": "sha256:80a184fe7e6f845fddc621ccba2abe0cbfdf46df843dba22c0cfb4b8ca11f97e"
 }
}